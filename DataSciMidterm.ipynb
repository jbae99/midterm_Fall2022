{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNFBWsGzi3oH0pGzVlo4u/w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbae99/midterm_Fall2022/blob/main/DataSciMidterm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--GgNJPYGtZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d799fe5-389b-4b1f-fff2-5e70827fd26d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installations\n",
        "!pip install -U scikit-learn\n",
        "!pip install pandas\n",
        "#for data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#for stats tests\n",
        "import scipy as sp\n",
        "#for plotting\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "#for machine learning\n",
        "from sklearn import preprocessing, model_selection, feature_selection, ensemble, linear_model, metrics, decomposition, neighbors, svm, naive_bayes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import make_column_selector as selector\n",
        "#for metric evaluations\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "PJNxexGrGy59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c12a1db-b116-4560-e3c2-21d17fe615d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLModels:\n",
        "  ##global attributes for conveinence (encoder is global as it is used in several methods)\n",
        "  scalerTypes = ['standard', 'minmax', 'robust']\n",
        "  algorithms = ['GaussNB', 'Perceptron', 'SVM']\n",
        "  __encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "  ##constructor method opens, assigns, and gives a basic summary of data file given in parameter\n",
        "  ##also makes a basic barplot of data \n",
        "  def __init__(self, dataPath):\n",
        "    dataset = pd.read_csv(dataPath, delimiter=';',comment='#')\n",
        "    self.x = dataset[['Signi070', 'Sp070', 'e_Sp070',\n",
        "       'Sp070/Sbg070', 'Sconv070', 'Stot070', 'e_Stot070', 'FWHMa070',\n",
        "       'FWHMb070', 'PA070', 'Signi160', 'Sp160', 'e_Sp160', 'Sp160/Sbg160',\n",
        "       'Sconv160', 'Stot160', 'e_Stot160', 'FWHMa160', 'FWHMb160', 'PA160',\n",
        "       'Signi250', 'Sp250', 'e_Sp250', 'Sp250/Sbg250', 'Sconv250', 'Stot250',\n",
        "       'e_Stot250', 'FWHMa250', 'FWHMb250', 'PA250', 'Signi350', 'Sp350',\n",
        "       'e_Sp350', 'Sp350/Sbg350', 'Sconv350', 'Stot350', 'e_Stot350',\n",
        "       'FWHMa350', 'FWHMb350', 'PA350', 'Signi500', 'Sp500', 'e_Sp500',\n",
        "       'Sp500/Sbg500', 'Stot500', 'e_Stot500', 'FWHMa500', 'FWHMb500', 'PA500',\n",
        "       'SigniNH2', 'NpH2', 'NpH2/Nbg', 'NconvH2', 'NbgH2', 'FWHMaNH2',\n",
        "       'FWHMbNH2', 'PANH2', 'NSED']]\n",
        "    self.y = dataset['Coretype']\n",
        "  \n",
        "    print(f'Summary Stats of Loaded Dataframe \\n {self.x.describe} \\n {self.y.describe}')\n",
        "    \n",
        "\n",
        "    #stripping whitespace and replacing empty values with NaN\n",
        "    cat_col_sel = selector(dtype_include = object)\n",
        "    for col in cat_col_sel(self.x):\n",
        "      self.x[col].str.strip()\n",
        "    self.x = self.x.replace(r'^\\s*$', np.nan, regex=True)\n",
        "\n",
        "    #print summary statistics of dataframe\n",
        "    print(dataset.describe(include='all'))\n",
        "\n",
        "    #make a plot\n",
        "\n",
        "\n",
        "  ##method splits the data into a training set and testing set based on parameter\n",
        "  def splitTestTrain(self, ratio):\n",
        "    #split data and print shape of train and test values\n",
        "    self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x, self.y, test_size = ratio)\n",
        "    print(f'Shape of original Dataframe: {self.x.shape} {self.y.shape} \\n\\\n",
        "    Shape of training data: {self.x_train.shape} {self.y_train.shape} \\n\\\n",
        "    Shape of testing data: {self.x_test.shape} {self.y_test.shape}')\n",
        "\n",
        "    #imputing values in training and test data\n",
        "    imputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n",
        "    imputer.fit(self.x_train)\n",
        "    self.x_train = imputer.transform(self.x_train)\n",
        "    self.x_test = imputer.transform(self.x_test)\n",
        "\n",
        "    #encoding target (y) values\n",
        "    self.__encoder.fit(self.y_train)\n",
        "    self.y_train = self.__encoder.transform(self.y_train)\n",
        "    self.y_test = self.__encoder.transform(self.y_test)\n",
        "\n",
        "  ##method scales the data based on the parameter given\n",
        "  def scaleData(self, scaleType):\n",
        "    while scaleType not in self.scalerTypes:\n",
        "      scaleType = input(f'\\nPlease select a valid scaler type: {self.scalerTypes}')\n",
        "    \n",
        "    #selecting scaler object to utilize\n",
        "    if scaleType == 'standard':\n",
        "      self.__scaler = preprocessing.StandardScaler()\n",
        "    elif scaleType == 'minmax':\n",
        "      self.__scaler = preprocessing.MinMaxScaler()\n",
        "    elif scaleType == 'robust':\n",
        "      self.__scaler = preprocessing.RobustScaler()\n",
        "    \n",
        "    #scaling x data\n",
        "    print(f'Scaling test and training x data using {scaleType}')\n",
        "    self.__scaler.fit(self.x_train)\n",
        "    self.x_train = self.__scaler.transform(self.x_train)\n",
        "    self.x_test = self.__scaler.transform(self.x_test)\n",
        "    self.x_train_df = pd.DataFrame(self.x_train)\n",
        "    print(f'\\nSummary of dataframe scaled with {scaleType}:\\n{self.x_train_df.describe}')\n",
        "\n",
        "\n",
        "  ##method chooses the algorithm to use for the model based on parameter given\n",
        "\n",
        "  def classifyData(self, algorithm):\n",
        "    while algorithm not in self.algorithms:\n",
        "      algorithm = input(f'Please enter one of the available inputs: {self.algorithms}')\n",
        "\n",
        "    if algorithm == 'GaussNB':\n",
        "      GNB = GaussianNB()\n",
        "      GNB.fit(self.x_train, self.y_train)\n",
        "      self.y_pred = GNB.predict(self.x_test)\n",
        "\n",
        "    elif algorithm == 'Perceptron':\n",
        "      PT = Perceptron(tol = 0.001, random_state = 0)\n",
        "      PT.fit(self.x_train, self.y_train)\n",
        "      self.y_pred = PT.predict(self.x_test)\n",
        "\n",
        "    elif algorithm == 'SVM':\n",
        "      mySVM = svm.SVC()\n",
        "      mySVM.fit(self.x_train, self.y_train)\n",
        "      self.y_pred = mySVM.predict(self.x_test)\n",
        "\n",
        "\n",
        "  ##method analyzes predicted values generated from classifyData. Prints Confusion matrix, classification report, and overall accuracy of the algorithm\n",
        "  def showResults(self):\n",
        "\n",
        "    print(f'Confusion Matrix and full Classification Report: \\n{confusion_matrix(self.y_test, self.y_pred)}')\n",
        "    print(classification_report(self.y_test, self.y_pred)) \n",
        "\n",
        "    # Evaluate label (subsets) accuracy\n",
        "    print(f'Overall Accuracy of MLM: {accuracy_score(self.y_test, self.y_pred)}')\n",
        "\n",
        "    print(f'Predicted classes: {self.__encoder.inverse_transform(self.y_pred)} \\n\\\n",
        "    Actual classes: {self.__encoder.inverse_transform(self.y_test)}')"
      ],
      "metadata": {
        "id": "DvoVlUaQdTva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataPath = '/content/gdrive/MyDrive/Topics Data/ophiuchus_tablea1(1).tsv'\n",
        "\n",
        "\n",
        "##Create a MLModels object (loads the data from dataPath, splits x and y, and gives basic summary)\n",
        "myMachine = MLModels(dataPath)\n",
        "\n",
        "##split the data into training and testing\n",
        "myMachine.splitTestTrain(0.3)\n",
        "\n",
        "##scale the data (possible inputs are 'standard', 'minmax', and 'robust' (all found in myMachine.scaleTypes))\n",
        "myMachine.scaleData('robust')\n",
        "\n",
        "##fit models, predict outcomes, and display results of each model (available models found in myMachine.algorithms)\n",
        "for model in myMachine.algorithms:\n",
        "  print(f'Classifying data with {model}')\n",
        "  myMachine.classifyData(model)\n",
        "  print(f'Results of {model}')\n",
        "  myMachine.showResults()\n"
      ],
      "metadata": {
        "id": "lz6xYjK0G18N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492714f9-1bd4-404c-e347-99177f033d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary Stats of Loaded Dataframe \n",
            " <bound method NDFrame.describe of      Signi070    Sp070  e_Sp070  Sp070/Sbg070  Sconv070  Stot070  e_Stot070  \\\n",
            "0         1.6 -0.01810    0.015         -0.59   -0.0656 -0.00282      0.022   \n",
            "1         0.0  0.00595    0.015          0.03    0.1070 -0.77100      0.110   \n",
            "2        47.0  0.75800    0.017          3.55    0.8610  1.48000      0.038   \n",
            "3         0.4 -0.00339    0.015         -0.01   -0.0338  0.28400      0.027   \n",
            "4         0.0  0.00435    0.015          0.01    0.0830  4.38000      0.110   \n",
            "..        ...      ...      ...           ...       ...      ...        ...   \n",
            "508      48.6  0.76900    0.017          7.51    1.1900  1.18000      0.027   \n",
            "509       0.0  0.00517    0.015          0.06    0.0978  0.68600      0.120   \n",
            "510       0.0  0.00714    0.015          0.08    0.1310  0.72600      0.090   \n",
            "511       2.9 -0.01480    0.015         -0.22   -0.2350 -0.62300      0.033   \n",
            "512      39.3  0.51900    0.016          4.79    1.0700  1.28000      0.024   \n",
            "\n",
            "    FWHMa070 FWHMb070 PA070  ...  PA500  SigniNH2  NpH2  NpH2/Nbg  NconvH2  \\\n",
            "0         27       11    10  ...     76       9.7   0.6      0.30      0.2   \n",
            "1        107       89    30  ...    125      24.1   1.1      0.24      0.9   \n",
            "2          9        8    71  ...      7      50.8   2.2      0.37      0.6   \n",
            "3         51       26    51  ...    142      12.4   0.9      0.17      0.5   \n",
            "4        156      123    35  ...    103      13.1   1.0      0.23      0.8   \n",
            "..       ...      ...   ...  ...    ...       ...   ...       ...      ...   \n",
            "508       12        8   134  ...      2      47.2   3.4      0.90      0.8   \n",
            "509      134      124    21  ...    100      12.3   1.1      0.25      1.0   \n",
            "510      113      101   -24  ...    103      20.7   2.4      0.30      2.0   \n",
            "511       94       54    -4  ...     97     226.1  22.9      2.71     17.1   \n",
            "512       16        9   -31  ...     11       6.3   0.5      0.15      0.1   \n",
            "\n",
            "     NbgH2  FWHMaNH2 FWHMbNH2 PANH2 NSED  \n",
            "0      1.9        21       19    98    0  \n",
            "1      4.8        78       48   104    0  \n",
            "2      5.9        23       18    -7    3  \n",
            "3      5.1        45       29   143    0  \n",
            "4      4.4        87       59   126    0  \n",
            "..     ...       ...      ...   ...  ...  \n",
            "508    3.7        18       18    17    3  \n",
            "509    4.5        92       86   124    0  \n",
            "510    8.1        82       56   105    0  \n",
            "511    8.4        59       50    93    1  \n",
            "512    3.2        24       18    -8    0  \n",
            "\n",
            "[513 rows x 58 columns]> \n",
            " <bound method NDFrame.describe of 0          starless\n",
            "1          starless\n",
            "2      protostellar\n",
            "3          starless\n",
            "4          starless\n",
            "           ...     \n",
            "508    protostellar\n",
            "509        starless\n",
            "510        starless\n",
            "511      prestellar\n",
            "512        starless\n",
            "Name: Coretype, Length: 513, dtype: object>\n",
            "               Seq             Name      RAJ2000      DEJ2000      Signi070  \\\n",
            "count   513.000000              513          513          513    513.000000   \n",
            "unique         NaN              513          509          511           NaN   \n",
            "top            NaN  162035.2-231721  16 27 56.71  -24 51 00.1           NaN   \n",
            "freq           NaN                1            2            2           NaN   \n",
            "mean    257.000000              NaN          NaN          NaN     62.058480   \n",
            "std     148.234611              NaN          NaN          NaN    593.758981   \n",
            "min       1.000000              NaN          NaN          NaN      0.000000   \n",
            "25%     129.000000              NaN          NaN          NaN      0.000000   \n",
            "50%     257.000000              NaN          NaN          NaN      0.000000   \n",
            "75%     385.000000              NaN          NaN          NaN      4.900000   \n",
            "max     513.000000              NaN          NaN          NaN  11910.000000   \n",
            "\n",
            "             Sp070     e_Sp070  Sp070/Sbg070    Sconv070     Stot070  ...  \\\n",
            "count   513.000000  513.000000    513.000000  513.000000  513.000000  ...   \n",
            "unique         NaN         NaN           NaN         NaN         NaN  ...   \n",
            "top            NaN         NaN           NaN         NaN         NaN  ...   \n",
            "freq           NaN         NaN           NaN         NaN         NaN  ...   \n",
            "mean      0.766368    0.041686      1.769805    1.499758    2.398127  ...   \n",
            "std       6.470035    0.146020     13.575532   13.409242   16.421507  ...   \n",
            "min      -0.349000    0.015000     -0.880000   -4.570000   -3.580000  ...   \n",
            "25%      -0.007950    0.015000     -0.020000   -0.037800   -0.043100  ...   \n",
            "50%       0.013100    0.015000      0.040000    0.106000    0.119000  ...   \n",
            "75%       0.041200    0.021000      0.150000    0.401000    0.831000  ...   \n",
            "max     114.000000    2.600000    218.290000  272.000000  301.000000  ...   \n",
            "\n",
            "           SigniNH2        NpH2    NpH2/Nbg     NconvH2       NbgH2  \\\n",
            "count    513.000000  513.000000  513.000000  513.000000  513.000000   \n",
            "unique          NaN         NaN         NaN         NaN         NaN   \n",
            "top             NaN         NaN         NaN         NaN         NaN   \n",
            "freq            NaN         NaN         NaN         NaN         NaN   \n",
            "mean      68.387719    5.518519    0.330897    2.219298   11.233528   \n",
            "std      343.624028   26.285607    0.591996    7.896192    8.354752   \n",
            "min        6.200000    0.200000    0.030000    0.100000    1.400000   \n",
            "25%       11.000000    0.700000    0.110000    0.300000    5.800000   \n",
            "50%       20.000000    1.400000    0.180000    0.600000    9.100000   \n",
            "75%       46.400000    3.500000    0.320000    1.600000   13.800000   \n",
            "max     6877.000000  497.400000    9.590000  125.000000   59.900000   \n",
            "\n",
            "          FWHMaNH2    FWHMbNH2       PANH2        NSED  Coretype  \n",
            "count   513.000000  513.000000  513.000000  513.000000       513  \n",
            "unique         NaN         NaN         NaN         NaN         3  \n",
            "top            NaN         NaN         NaN         NaN  starless  \n",
            "freq           NaN         NaN         NaN         NaN       320  \n",
            "mean     39.366472   25.313840   47.103314    0.590643       NaN  \n",
            "std      19.434943   10.588399   53.005150    1.189172       NaN  \n",
            "min      18.000000   18.000000  -36.000000    0.000000       NaN  \n",
            "25%      27.000000   18.000000   -1.000000    0.000000       NaN  \n",
            "50%      35.000000   21.000000   41.000000    0.000000       NaN  \n",
            "75%      46.000000   28.000000   92.000000    0.000000       NaN  \n",
            "max     228.000000   86.000000  143.000000    4.000000       NaN  \n",
            "\n",
            "[11 rows x 63 columns]\n",
            "Shape of original Dataframe: (513, 58) (513,) \n",
            "    Shape of training data: (359, 58) (359,) \n",
            "    Shape of testing data: (154, 58) (154,)\n",
            "Scaling test and training x data using robust\n",
            "\n",
            "Summary of dataframe scaled with robust:\n",
            "<bound method NDFrame.describe of            0         1         2         3         4         5         6   \\\n",
            "0    0.000000  0.002161  0.333333  0.171429  0.023051 -0.106226 -0.112903   \n",
            "1    0.000000  0.010804  0.500000  0.000000  0.319754 -0.330637  1.177419   \n",
            "2    0.421053 -0.687122  0.000000 -3.142857 -0.698163 -0.162973 -0.096774   \n",
            "3    0.000000  0.998271  0.000000  6.400000  0.965651  0.001172 -0.193548   \n",
            "4    0.000000 -0.238116  0.000000 -0.171429 -0.205067 -0.072576 -0.500000   \n",
            "..        ...       ...       ...       ...       ...       ...       ...   \n",
            "354  0.000000  0.181504  0.000000  1.257143  0.691772 -0.170243  1.177419   \n",
            "355  0.631579 -0.671997  0.000000 -1.085714 -0.380692 -0.146043 -0.516129   \n",
            "356  0.000000  0.071305  0.166667  0.171429  0.235308 -0.653066  0.080645   \n",
            "357  0.605263 -0.708729  0.000000 -0.971429 -0.689033 -0.131833 -0.290323   \n",
            "358  1.815789 -0.769231  0.000000 -2.171429 -0.705010 -0.178684 -0.516129   \n",
            "\n",
            "           7         8         9   ...        48        49        50  \\\n",
            "0   -0.454545  0.158730 -0.358382  ...  0.388571 -0.370370 -0.275862   \n",
            "1    1.606061  1.777778 -0.751445  ...  0.000000  0.000000  0.068966   \n",
            "2    0.151515  0.285714  0.913295  ... -0.822857 -0.105413 -0.034483   \n",
            "3   -0.303030  0.126984  0.219653  ... -0.514286 -0.321937 -0.344828   \n",
            "4   -0.454545 -0.476190  1.086705  ...  0.937143 -0.005698 -0.103448   \n",
            "..        ...       ...       ...  ...       ...       ...       ...   \n",
            "354  2.212121  2.920635 -0.289017  ... -0.560000  0.056980  0.241379   \n",
            "355 -0.303030 -0.412698 -0.289017  ...  0.308571 -0.384615 -0.379310   \n",
            "356  0.181818  0.444444 -0.289017  ... -0.937143 -0.162393 -0.103448   \n",
            "357  0.000000  0.222222  0.971098  ...  0.537143  0.062678  0.068966   \n",
            "358  0.363636  0.000000 -0.763006  ...  0.434286 -0.056980 -0.206897   \n",
            "\n",
            "           51    52        53        54        55        56   57  \n",
            "0   -0.564103 -0.32  0.104575 -0.864865 -0.333333  1.011236  0.0  \n",
            "1   -0.153846  0.40  0.261438  0.756757  1.555556  0.370787  0.0  \n",
            "2    1.794872  0.08 -0.849673  0.162162  0.444444 -0.494382  0.0  \n",
            "3    0.769231 -0.40 -1.006536 -0.648649 -0.333333 -0.516854  0.0  \n",
            "4    0.000000 -0.08 -0.274510 -0.054054 -0.333333 -0.056180  0.0  \n",
            "..        ...   ...       ...       ...       ...       ...  ...  \n",
            "354  0.769231  0.88 -0.339869  2.324324  3.555556 -0.303371  0.0  \n",
            "355 -0.666667 -0.32 -0.130719 -0.324324  0.000000  0.629213  0.0  \n",
            "356 -0.358974 -0.08  0.261438 -0.324324  0.000000 -0.595506  0.0  \n",
            "357  0.256410  0.16 -0.248366  0.432432  0.444444  0.640449  0.0  \n",
            "358 -0.051282 -0.08 -0.444444  1.081081  0.111111  0.539326  0.0  \n",
            "\n",
            "[359 rows x 58 columns]>\n",
            "Classifying data with GaussNB\n",
            "Results of GaussNB\n",
            "Confusion Matrix and full Classification Report: \n",
            "[[26  1 17]\n",
            " [ 2 10  3]\n",
            " [ 1  3 91]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.59      0.71        44\n",
            "           1       0.71      0.67      0.69        15\n",
            "           2       0.82      0.96      0.88        95\n",
            "\n",
            "    accuracy                           0.82       154\n",
            "   macro avg       0.81      0.74      0.76       154\n",
            "weighted avg       0.83      0.82      0.82       154\n",
            "\n",
            "Overall Accuracy of MLM: 0.8246753246753247\n",
            "Predicted classes: ['prestellar' 'prestellar' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'protostellar' 'protostellar' 'starless'\n",
            " 'starless' 'protostellar' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'prestellar' 'prestellar' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'protostellar' 'protostellar' 'starless' 'starless' 'starless'\n",
            " 'protostellar' 'prestellar' 'starless' 'starless' 'prestellar'\n",
            " 'protostellar' 'starless' 'prestellar' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'prestellar' 'protostellar' 'starless' 'starless'\n",
            " 'starless' 'prestellar' 'starless' 'starless' 'prestellar' 'starless'\n",
            " 'protostellar' 'prestellar' 'protostellar' 'starless' 'prestellar'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'prestellar' 'starless' 'starless' 'prestellar' 'prestellar'\n",
            " 'prestellar' 'starless' 'prestellar' 'starless' 'prestellar' 'prestellar'\n",
            " 'starless' 'starless' 'prestellar' 'starless' 'protostellar' 'starless'\n",
            " 'starless' 'prestellar' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'prestellar' 'protostellar' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'prestellar' 'starless'\n",
            " 'starless' 'starless' 'starless' 'protostellar' 'starless' 'starless'\n",
            " 'protostellar' 'starless' 'starless' 'starless' 'prestellar' 'prestellar'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'prestellar' 'prestellar' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'prestellar' 'starless' 'prestellar'] \n",
            "    Actual classes: ['prestellar' 'prestellar' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'protostellar' 'protostellar' 'starless'\n",
            " 'protostellar' 'protostellar' 'starless' 'prestellar' 'starless'\n",
            " 'starless' 'prestellar' 'starless' 'starless' 'prestellar' 'prestellar'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'prestellar' 'starless' 'starless'\n",
            " 'starless' 'starless' 'protostellar' 'protostellar' 'starless' 'starless'\n",
            " 'starless' 'protostellar' 'prestellar' 'starless' 'starless' 'prestellar'\n",
            " 'prestellar' 'starless' 'prestellar' 'starless' 'protostellar' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'prestellar' 'protostellar' 'starless' 'starless'\n",
            " 'starless' 'protostellar' 'starless' 'starless' 'prestellar' 'starless'\n",
            " 'starless' 'prestellar' 'starless' 'prestellar' 'prestellar' 'starless'\n",
            " 'prestellar' 'starless' 'prestellar' 'starless' 'starless' 'starless'\n",
            " 'protostellar' 'starless' 'prestellar' 'prestellar' 'prestellar'\n",
            " 'prestellar' 'starless' 'prestellar' 'prestellar' 'prestellar'\n",
            " 'prestellar' 'starless' 'starless' 'prestellar' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'prestellar' 'starless' 'prestellar'\n",
            " 'prestellar' 'starless' 'starless' 'prestellar' 'prestellar'\n",
            " 'protostellar' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'prestellar' 'starless' 'prestellar' 'starless' 'prestellar'\n",
            " 'protostellar' 'starless' 'starless' 'protostellar' 'protostellar'\n",
            " 'starless' 'starless' 'prestellar' 'prestellar' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'prestellar'\n",
            " 'prestellar' 'prestellar' 'starless' 'starless' 'prestellar' 'starless'\n",
            " 'starless' 'prestellar' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'prestellar' 'starless' 'prestellar']\n",
            "Classifying data with Perceptron\n",
            "Results of Perceptron\n",
            "Confusion Matrix and full Classification Report: \n",
            "[[37  0  7]\n",
            " [ 0 14  1]\n",
            " [ 4  3 88]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.84      0.87        44\n",
            "           1       0.82      0.93      0.87        15\n",
            "           2       0.92      0.93      0.92        95\n",
            "\n",
            "    accuracy                           0.90       154\n",
            "   macro avg       0.88      0.90      0.89       154\n",
            "weighted avg       0.90      0.90      0.90       154\n",
            "\n",
            "Overall Accuracy of MLM: 0.9025974025974026\n",
            "Predicted classes: ['prestellar' 'prestellar' 'starless' 'starless' 'starless' 'protostellar'\n",
            " 'starless' 'starless' 'starless' 'protostellar' 'protostellar' 'starless'\n",
            " 'protostellar' 'protostellar' 'starless' 'prestellar' 'starless'\n",
            " 'starless' 'prestellar' 'starless' 'starless' 'prestellar' 'prestellar'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'prestellar' 'starless' 'starless'\n",
            " 'starless' 'starless' 'protostellar' 'protostellar' 'starless' 'starless'\n",
            " 'starless' 'protostellar' 'prestellar' 'starless' 'prestellar'\n",
            " 'prestellar' 'prestellar' 'starless' 'starless' 'starless' 'protostellar'\n",
            " 'starless' 'starless' 'starless' 'starless' 'prestellar' 'starless'\n",
            " 'starless' 'starless' 'starless' 'prestellar' 'protostellar' 'starless'\n",
            " 'starless' 'starless' 'protostellar' 'starless' 'starless' 'prestellar'\n",
            " 'starless' 'starless' 'prestellar' 'starless' 'prestellar' 'prestellar'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'prestellar' 'prestellar' 'prestellar'\n",
            " 'prestellar' 'prestellar' 'prestellar' 'starless' 'prestellar'\n",
            " 'prestellar' 'starless' 'starless' 'prestellar' 'starless' 'protostellar'\n",
            " 'starless' 'starless' 'starless' 'prestellar' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'prestellar' 'prestellar' 'protostellar'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'prestellar'\n",
            " 'starless' 'prestellar' 'starless' 'prestellar' 'protostellar' 'starless'\n",
            " 'starless' 'protostellar' 'protostellar' 'protostellar' 'prestellar'\n",
            " 'prestellar' 'prestellar' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'prestellar' 'prestellar' 'prestellar'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'prestellar'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'prestellar'\n",
            " 'starless' 'prestellar'] \n",
            "    Actual classes: ['prestellar' 'prestellar' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'protostellar' 'protostellar' 'starless'\n",
            " 'protostellar' 'protostellar' 'starless' 'prestellar' 'starless'\n",
            " 'starless' 'prestellar' 'starless' 'starless' 'prestellar' 'prestellar'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'prestellar' 'starless' 'starless'\n",
            " 'starless' 'starless' 'protostellar' 'protostellar' 'starless' 'starless'\n",
            " 'starless' 'protostellar' 'prestellar' 'starless' 'starless' 'prestellar'\n",
            " 'prestellar' 'starless' 'prestellar' 'starless' 'protostellar' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'prestellar' 'protostellar' 'starless' 'starless'\n",
            " 'starless' 'protostellar' 'starless' 'starless' 'prestellar' 'starless'\n",
            " 'starless' 'prestellar' 'starless' 'prestellar' 'prestellar' 'starless'\n",
            " 'prestellar' 'starless' 'prestellar' 'starless' 'starless' 'starless'\n",
            " 'protostellar' 'starless' 'prestellar' 'prestellar' 'prestellar'\n",
            " 'prestellar' 'starless' 'prestellar' 'prestellar' 'prestellar'\n",
            " 'prestellar' 'starless' 'starless' 'prestellar' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'prestellar' 'starless' 'prestellar'\n",
            " 'prestellar' 'starless' 'starless' 'prestellar' 'prestellar'\n",
            " 'protostellar' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'prestellar' 'starless' 'prestellar' 'starless' 'prestellar'\n",
            " 'protostellar' 'starless' 'starless' 'protostellar' 'protostellar'\n",
            " 'starless' 'starless' 'prestellar' 'prestellar' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'prestellar'\n",
            " 'prestellar' 'prestellar' 'starless' 'starless' 'prestellar' 'starless'\n",
            " 'starless' 'prestellar' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'prestellar' 'starless' 'prestellar']\n",
            "Classifying data with SVM\n",
            "Results of SVM\n",
            "Confusion Matrix and full Classification Report: \n",
            "[[ 5  0 39]\n",
            " [ 0  9  6]\n",
            " [ 0  1 94]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.11      0.20        44\n",
            "           1       0.90      0.60      0.72        15\n",
            "           2       0.68      0.99      0.80        95\n",
            "\n",
            "    accuracy                           0.70       154\n",
            "   macro avg       0.86      0.57      0.58       154\n",
            "weighted avg       0.79      0.70      0.62       154\n",
            "\n",
            "Overall Accuracy of MLM: 0.7012987012987013\n",
            "Predicted classes: ['prestellar' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'protostellar' 'protostellar' 'starless'\n",
            " 'starless' 'protostellar' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'prestellar' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'protostellar' 'protostellar' 'starless' 'starless' 'starless'\n",
            " 'protostellar' 'starless' 'starless' 'starless' 'starless' 'prestellar'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'prestellar' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'prestellar' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'protostellar' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'protostellar' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'protostellar' 'starless' 'starless' 'protostellar' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless'] \n",
            "    Actual classes: ['prestellar' 'prestellar' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'protostellar' 'protostellar' 'starless'\n",
            " 'protostellar' 'protostellar' 'starless' 'prestellar' 'starless'\n",
            " 'starless' 'prestellar' 'starless' 'starless' 'prestellar' 'prestellar'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'prestellar' 'starless' 'starless'\n",
            " 'starless' 'starless' 'protostellar' 'protostellar' 'starless' 'starless'\n",
            " 'starless' 'protostellar' 'prestellar' 'starless' 'starless' 'prestellar'\n",
            " 'prestellar' 'starless' 'prestellar' 'starless' 'protostellar' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'starless' 'prestellar' 'protostellar' 'starless' 'starless'\n",
            " 'starless' 'protostellar' 'starless' 'starless' 'prestellar' 'starless'\n",
            " 'starless' 'prestellar' 'starless' 'prestellar' 'prestellar' 'starless'\n",
            " 'prestellar' 'starless' 'prestellar' 'starless' 'starless' 'starless'\n",
            " 'protostellar' 'starless' 'prestellar' 'prestellar' 'prestellar'\n",
            " 'prestellar' 'starless' 'prestellar' 'prestellar' 'prestellar'\n",
            " 'prestellar' 'starless' 'starless' 'prestellar' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'prestellar' 'starless' 'prestellar'\n",
            " 'prestellar' 'starless' 'starless' 'prestellar' 'prestellar'\n",
            " 'protostellar' 'starless' 'starless' 'starless' 'starless' 'starless'\n",
            " 'prestellar' 'starless' 'prestellar' 'starless' 'prestellar'\n",
            " 'protostellar' 'starless' 'starless' 'protostellar' 'protostellar'\n",
            " 'starless' 'starless' 'prestellar' 'prestellar' 'starless' 'starless'\n",
            " 'starless' 'starless' 'starless' 'starless' 'starless' 'prestellar'\n",
            " 'prestellar' 'prestellar' 'starless' 'starless' 'prestellar' 'starless'\n",
            " 'starless' 'prestellar' 'starless' 'starless' 'starless' 'starless'\n",
            " 'starless' 'prestellar' 'starless' 'prestellar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ygAhr9Z9lU9D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}